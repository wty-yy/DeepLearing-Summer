{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wNPOZG7qkBn"
   },
   "source": [
    "# Part2: Music Generation with RNNs\n",
    "\n",
    "此部分为 `Part2_Music_Generation.ipynb` 的化简版本，理清主要思路，只包含核心代码.\n",
    "\n",
    "1. 预处理数据集：\n",
    "- 构建单词库 `vacab`，以频率高低设置对应索引，将字符串转为数字.\n",
    "- 构建训练集batch，包含 `sequence_length` 和 `batch_size` 两个参数. 每个batch中的样本序列的开头 `start` 为 `[0,n-len-1]` 中随机选取的，其中 `n=vacab_size` 词库大小. 每个样本的特征为数据集的子串 `[start, start+len-1]`，标签为子串 `[start+1, start+len]`.\n",
    "\n",
    "2. 搭建模型：embedding层，参数 `embedding_dimensionality` $\\to$ LSTM层，参数 `rnn_units` $\\to$ Dense层，参数 `units=vacab_size`.\n",
    "\n",
    "3. 定义损失函数，使用交叉熵函数. 超参数配置：`training_iterations`，`learning_rate`. 构建训练函数：\n",
    "- 使用 `tf.GradientTap` 对变量进行观测，计算 $\\mathcal{L}(y, \\hat{y})$.\n",
    "- 求出 $\\frac{\\partial\\mathcal{L}}{\\partial W}$，$W$ 为全体可学习参数 `model.trainable_variables`.\n",
    "- 使用 `optimizer` 对梯度进行更新.\n",
    "- 开始训练：执行训练函数 `training_iterations` 次，用 `tqdm` 可视化进度条，在记录点保存模型.\n",
    "\n",
    "4. 生成歌曲，根据启动种子 `start_text` 作为预测序列的开头，用 `tf.random.categorical` 以输出的结果作为概率分布选出一个预测值，作为下一次预测的输入值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import keras.layers as layers\n",
    "import myTools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mitdeeplearning as mdl\n",
    "import IPython.display as ipythondisplay\n",
    "import pathlib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = mdl.lab1.load_training_data()\n",
    "dataset = '\\n\\n'.join(x[1] for x in enumerate(songs))\n",
    "vocab = set(dataset)\n",
    "vocab_size = len(vocab)\n",
    "cnt = {}\n",
    "for c in vocab:\n",
    "    cnt[c] = dataset.count(c)\n",
    "cnt = sorted(cnt.items(), key=lambda x: x[1], reverse=True)\n",
    "vocab = [x[0] for x in cnt]  # 构建出现频率从大到小的字符库\n",
    "# 构建两个对应表\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "# 数字化dataset\n",
    "dataset = [char2idx[c] for c in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(ds, seq_length, batch_size):\n",
    "    n = len(ds)\n",
    "    idx = np.random.choice(n - seq_length, batch_size)\n",
    "    x = np.array([ds[st:st+seq_length] for st in idx])\n",
    "    y = np.array([ds[st+1:st+seq_length+1] for st in idx])\n",
    "    return x, y\n",
    "\n",
    "example = make_batch(dataset, 10, 2)\n",
    "print(example[0].shape, example[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    return keras.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim),\n",
    "        layers.LSTM(rnn_units, return_sequences=True),\n",
    "        layers.Dense(vocab_size)\n",
    "    ])\n",
    "\n",
    "def compute_loss(y, y_hat):\n",
    "    return keras.losses.sparse_categorical_crossentropy(y, y_hat, from_logits=True)\n",
    "\n",
    "# Hyperparameters setup\n",
    "training_iterations = 5000\n",
    "batch_size = 32\n",
    "seq_length = 100\n",
    "learning_rate = 5e-3\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "checkpoint_dir = pathlib.Path('./training_checkpoints')\n",
    "checkpoint_prefix = checkpoint_dir.joinpath('my_ckpt')\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)\n",
    "        loss = compute_loss(y, y_hat)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')  # 绘制loss函数动态变化图\n",
    "\n",
    "for _ in tqdm(range(training_iterations)):\n",
    "    x, y = make_batch(dataset, seq_length, batch_size)\n",
    "    loss = train_step(x, y)\n",
    "    history.append(loss.numpy().mean())\n",
    "    plotter.plot(history)\n",
    "    if _ % 100 == 0:\n",
    "        model.save_weights(checkpoint_prefix)\n",
    "model.save_weights(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, length=1000):\n",
    "    x = np.array([char2idx[c] for c in start_text])\n",
    "    ret = []\n",
    "    model.reset_states()\n",
    "    for i in tqdm(range(length)):\n",
    "        pred = model(x.reshape(1, -1))\n",
    "        pred = tf.squeeze(pred, 0)\n",
    "        pred_id = tf.random.categorical(pred, num_samples=1)[-1, 0].numpy()\n",
    "        x = np.array([pred_id])\n",
    "        ret.append(idx2char[pred_id])\n",
    "    return start_text + ''.join(ret)\n",
    "mysong = generate_text(model, 'X')\n",
    "myTools.play_song(mysong)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part2 Music Generation.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
